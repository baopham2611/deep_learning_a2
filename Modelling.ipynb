{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "<a id=\"data-prep\"></a>\n",
    "<div style=\"background-color: #000D5B; color: white; text-align: center; padding: 6px 0 22px 0\">\n",
    "    <h3 style=\"background-color: #000D5B; color: white; text-align: left\">RMIT School of Computer Science and Technology</h3>\n",
    "    <br/>\n",
    "    <h1>COSC3007: Deep Learning</h1>\n",
    "    <h2>Assignment 2: Stance Twitter Sentiment Analysis and Detection </h2>\n",
    "    <h2> [2] MODELLING AND MODEL EVALUATIONS </h2>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./StanceDataset/processed_train.csv\", encoding = \"ISO-8859-1\", engine=\"python\").drop(columns=['Unnamed: 0'])\n",
    "test_df = pd.read_csv(\"./StanceDataset/processed_test.csv\", encoding = \"ISO-8859-1\", engine=\"python\").drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Prepare label and fit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique tokens - 7518\n",
      "vocab_size - 7519\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 123\n",
    "\n",
    "# Train\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['processed_tweet'])\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['processed_tweet'])\n",
    "\n",
    "# Test\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(train_df['processed_tweet'])\n",
    "test_sequences = tokenizer2.texts_to_sequences(test_df['processed_tweet'])\n",
    "\n",
    "# Val\n",
    "tokenizer3 = Tokenizer()\n",
    "tokenizer3.fit_on_texts(train_df['processed_tweet'])\n",
    "val_sequences = tokenizer3.texts_to_sequences(val_df['processed_tweet'])\n",
    "\n",
    "train_pad = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "test_pad = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "val_pad = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"unique tokens - \"+str(len(word_index)))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('vocab_size - '+str(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorized_label(df, label_name):\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(df[label_name])\n",
    "    categorical_labels = to_categorical(encoded_labels)\n",
    "    return categorical_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = categorized_label(train_df, \"Stance\")\n",
    "test_labels = categorized_label(test_df, \"Stance\")\n",
    "val_labels = categorized_label(val_df, \"Stance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the F1 score metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Calculate Precision and Recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up call backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Define the EarlyStopping and ReduceLROnPlateau callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_path = \"best_model2.h5\"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,    # Only save a model if 'val_loss' has improved\n",
    "    monitor='val_loss',     # Monitor 'val_loss' during training\n",
    "    mode='min',             # The model is saved when 'val_loss' is minimized\n",
    "    verbose=1)\n",
    "\n",
    "# Combine all callbacks in a list\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    lr_scheduler,\n",
    "    # model_checkpoint_callback\n",
    "]\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=16, input_length=MAX_LENGTH))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes: FAVOUR, AGAINST, NEITHER\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 11:32:31.931205: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/92 [======>.......................] - ETA: 41:14 - loss: 1.0792 - accuracy: 0.4592 - f1_score: 0.0124"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_pad, train_labels, epochs=10, validation_data=(val_pad, val_labels), batch_size=32, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
