{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "<a id=\"data-prep\"></a>\n",
    "<div style=\"background-color: #000D5B; color: white; text-align: center; padding: 6px 0 22px 0\">\n",
    "    <h3 style=\"background-color: #000D5B; color: white; text-align: left\">RMIT School of Computer Science and Technology</h3>\n",
    "    <br/>\n",
    "    <h1>COSC3007: Deep Learning</h1>\n",
    "    <h2>Assignment 2: Stance Twitter Sentiment Analysis and Detection </h2>\n",
    "    <h2> [2] MODELLING AND MODEL EVALUATIONS </h2>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [1] Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./StanceDataset/processed_train.csv\", encoding = \"ISO-8859-1\", engine=\"python\").drop(columns=['Unnamed: 0'])\n",
    "test_df = pd.read_csv(\"./StanceDataset/processed_test.csv\", encoding = \"ISO-8859-1\", engine=\"python\").drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [2] Prepare label and fit data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens: 7518\n",
      "Vocab size: 7519\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 123\n",
    "\n",
    "# Initialize and fit the tokenizer on the training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['processed_tweet'])\n",
    "\n",
    "# Convert texts to sequences for all datasets\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['processed_tweet'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['processed_tweet'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df['processed_tweet'])\n",
    "\n",
    "# Pad the sequences\n",
    "train_pad = pad_sequences(train_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "test_pad = pad_sequences(test_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "val_pad = pad_sequences(val_sequences, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "# Extract the word index and vocab size\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Unique tokens: \" + str(len(word_index)))\n",
    "vocab_size = len(word_index) + 1\n",
    "print('Vocab size: ' + str(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorized_label(df, label_name):\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(df[label_name])\n",
    "    categorical_labels = to_categorical(encoded_labels)\n",
    "    return categorical_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = categorized_label(train_df, \"Stance\")\n",
    "test_labels = categorized_label(test_df, \"Stance\")\n",
    "val_labels = categorized_label(val_df, \"Stance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [3] Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the F1 score metric\n",
    "def f1_score(y_true, y_pred):\n",
    "    # Calculate Precision and Recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up call backs and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Define the EarlyStopping and ReduceLROnPlateau callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=50,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=5,\n",
    "    min_lr=1e-5,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_path = \"best_model2.h5\"\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_best_only=True,    # Only save a model if 'val_loss' has improved\n",
    "    monitor='val_loss',     # Monitor 'val_loss' during training\n",
    "    mode='min',             # The model is saved when 'val_loss' is minimized\n",
    "    verbose=1)\n",
    "\n",
    "# Combine all callbacks in a list\n",
    "callbacks = [\n",
    "    early_stopping,\n",
    "    lr_scheduler,\n",
    "    # model_checkpoint_callback\n",
    "]\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 12:31:52.116632: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Max\n",
      "2023-12-20 12:31:52.116652: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n",
      "2023-12-20 12:31:52.116657: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n",
      "2023-12-20 12:31:52.116689: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-20 12:31:52.116704: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=16, input_length=MAX_LENGTH))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes: FAVOUR, AGAINST, NEITHER\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy', f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 12:31:53.224186: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/92 [===============>..............] - ETA: 23:44 - loss: 1.0904 - accuracy: 0.4099 - f1_score: 0.0185"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_pad, train_labels, \n",
    "                    epochs=20, \n",
    "                    validation_data=(val_pad, val_labels), \n",
    "                    batch_size=32, \n",
    "                    callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
